<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>Intelligent Artifice - Deep RL</title>
        <link rel="stylesheet" href="http://www.mikelanzetta.com/theme/css/main.css" />
        <link href="http://www.mikelanzetta.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Intelligent Artifice Atom Feed" />

        <!--[if IE]>
            <script src="https://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
<a href="https://github.com/noodlefrenzy">
<img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png" alt="Fork me on GitHub" />
</a>
        <header id="banner" class="body">
                <h1><a href="http://www.mikelanzetta.com/">Intelligent Artifice </a></h1>
                <nav><ul>
                    <li><a href="http://www.mikelanzetta.com/pages/about-me-and-this-blog.html">About Me And This Blog</a></li>
                    <li><a href="http://www.mikelanzetta.com/category/devops.html">DevOps</a></li>
                    <li><a href="http://www.mikelanzetta.com/category/meta.html">Meta</a></li>
                    <li><a href="http://www.mikelanzetta.com/category/software.html">Software</a></li>
                    <li><a href="http://www.mikelanzetta.com/category/uncategorized.html">Uncategorized</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="http://www.mikelanzetta.com/nips-2016-trip-report.html">NIPS 2016 Trip Report</a></h1>
<footer class="post-info">
        <abbr class="published" title="2016-12-13T11:52:00-08:00">
                Published: Tue 13 December 2016
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="http://www.mikelanzetta.com/author/noodlefrenzy.html">noodlefrenzy</a>
        </address>
<p>In <a href="http://www.mikelanzetta.com/category/software.html">Software</a>.</p>
<p>tags: <a href="http://www.mikelanzetta.com/tag/ml.html">ML</a> <a href="http://www.mikelanzetta.com/tag/machine-learning.html">Machine Learning</a> <a href="http://www.mikelanzetta.com/tag/deep-learning.html">Deep Learning</a> <a href="http://www.mikelanzetta.com/tag/gan.html">GAN</a> <a href="http://www.mikelanzetta.com/tag/deep-rl.html">Deep RL</a> </p>
</footer><!-- /.post-info --><p><img alt="NIPS Logo" src="http://www.mikelanzetta.com/images/NipsLogoSmall.png" title="NIPS Logo" />
<a href="https://nips.cc">NIPS (Neural Information Processing Symposium)</a> is one of the two largest academic conferences in the Machine Learning world (the other being ICML). This year the conference was held in Barcelona, which worked out well for me because I was planning to attend a hackfest the following week in Milan, so I decided to go as an attendee. </p>
<p>The conference has doubled in size in the past year, with ~6000 attendees, so their advice to pick up your badge the day before the conference began proved to be quite wise.</p>
<p><a style='text-decoration: none; color: orange;'>
<img src="http://www.mikelanzetta.com/images/nips2016-registration-line.jpg" alt="Huge line at registration"/>
<div style="width:790px;text-align:center;">Picking up the day before == brilliant</div>
</a></p>
<h2>General Insights</h2>
<p>There were a few general insights I had during the conference. First - Google, Facebook, and OpenAI are the stars of the show - from the lanyard giving Google and Facebook pride-of-place, to the dueling Google Research and DeepMind booths, Google seems to be the dominant force. Deep Learning is still the star, but Tensor-based learning methods are generally only increasing in prominence - very little on "traditional" methods. Microsoft's booth had solid attendance, but our relatively confusing story (how many offerings do we have? is our Deep Learning system named CNTK or Cognitive Toolkit?) seemed to prevent "virality" compared to Google's TensorFlow or Facebook's Torch. I talked up CNTK v2 to a few people, but when they went to our booth to get stickers only "Microsoft Cognitive Toolkit" stickers could be found.</p>
<p><a style='text-decoration: none; color: orange'>
<img src="http://www.mikelanzetta.com/images/nips2016-google-research-booth.jpg" alt="Crowd around Google Research booth" style="float:left;margin-right:10px"/>
<img src="http://www.mikelanzetta.com/images/nips2016-microsoft-booth.jpg" alt="Crowd around Microsoft booth"/>
<div style="width:790px;text-align:center;">Google Research vs. Microsoft Booths</div>
</a></p>
<h2>Machine Learning Trends</h2>
<h3>Keynote Talks</h3>
<p><a href="https://en.wikipedia.org/wiki/Yann_LeCun">Yann LeCun</a> gave an interesting keynote on the need for unsupervised learning in general domains as the big open challenge for the future of ML. He called the talk <a href="https://drive.google.com/file/d/0BxKBnD5y2M8NREZod0tVdW5FLTQ/view">Predictive Learning</a>, and described the need for the model to maintain an accurate "world state" in order to begin reasoning in a more general sense.</p>
<p><a style='text-decoration: none; color: orange;'>
<img src="http://www.mikelanzetta.com/images/yanns-cake.png" alt="Cake as illustration of learning challenges"/>
<div style="width:790px;text-align:center;">Yann's now-famous cake</div>
</a></p>
<p>Drew Purves (former Microsoftie, now at DeepMind) gave a keynote where he talked about the launch of <a href="https://github.com/deepmind/lab">DeepMind's Lab</a> and more generally talked about the obligation of ML to help solve some of our most pressing environmental problems. He had some great illustrations in those slides, but I haven't seen him post them yet unfortunately.</p>
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Psyched so many at <a href="https://twitter.com/hashtag/nips2016?src=hash">#nips2016</a> loved illustrations from my <a href="https://twitter.com/DeepMindAI">@DeepMindAI</a> artist colleague Max Cant! Here&#39;s a decent version of the medley :) <a href="https://t.co/I7ShngYiMQ">pic.twitter.com/I7ShngYiMQ</a></p>&mdash; Drew Purves (@DrewPurves) <a href="https://twitter.com/DrewPurves/status/806427029306560512">December 7, 2016</a></blockquote>

<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

<p><a href="https://en.wikipedia.org/wiki/Kyle_Cranmer">Kyle Cranmer</a> presented on using ML models in the processing of data at CERN, and some of the unique challenges they face there. They deal with truly big data there, and do a substantial amount of pre-filtering in order to even scope the problem down to something surmountable (<a href="https://figshare.com/articles/NIPS_2016_Keynote_Machine_Learning_Likelihood_Free_Inference_in_Particle_Physics/4291565/1">slides</a> and <a href="https://cds.cern.ch/record/1541893">video</a>). With ~1PB of data per second, mountains of processors filter the incoming data to attempt to trap only "relevant" events (leading to "only" 100GB/s of data). His talk ranged over many different approaches so I encourage you to take a look at the slide deck. I'll just call out two interesting developments. First, <a href="http://diana-hep.org/carl/">CARL</a>, a module for "likelihood-free inference" in Python for using likelihood estimation techniques in model-training. Second, a <a href="https://github.com/lukasheinrich/weinberg-test">Jupyter Notebook</a> for generating high-energy physics simulations on the web. </p>
<h3>Generative Adversarial Networks</h3>
<p>Earlier this year at ICML the big trends were around understanding Deep Neural Nets and exploring Deep Reinforcement Learning. At NIPS Generative Adversarial Networks were the star of the show, with <a href="https://en.wikipedia.org/wiki/Yann_LeCun">Yann LeCun</a> saying in his keynote that they were the most important development in ML in the last ten to twenty years. Ian Goodfellow gave a great tutorial on GANs (see <a href="http://www.iangoodfellow.com/slides/2016-12-04-NIPS.pdf">slides here</a>), and the creators of <a href="https://arxiv.org/abs/1606.03657v1">InfoGAN</a> also gave a great talk.</p>
<p><a style='text-decoration: none; color: orange;'>
<img src="http://www.mikelanzetta.com/images/goodfellow-generative-taxonomy.png" alt="Taxonomy of generative models"/>
<div style="width:790px;text-align:center;">Great overview of generative models</div>
</a></p>
<p>One key takeaway was that there is really no good method at present for judging the "success" of GANs in many domains. Maximum likelihood optimization results in blurry images, or images that bear some resemblance to reality but are still far from real. This extends to other domains as well, and is one of the big open issues in the field. In addition, mode collapse is still an unsolved problem (where a GAN Generator learns to generate a single sample type, instead of a diversity, since that's enough to fool the Descriminator). There has been some progress in this area, but it's another open area of research for a more general solution.</p>
<p>There was a great talk going into the state of the art on the hacks that people are currently using to train GANs effectively by the folks at Facebook AI Research. If you're planning on working with GANs, I'd consider it required reading (<a href="https://github.com/soumith/ganhacks">GitHub here</a>).</p>
<h3>Deep Learning and Tensors</h3>
<p>In the Deep Learning space one of the star papers was on <a href="https://arxiv.org/abs/1610.09513v1">Phased LSTMs</a>, which added a time-based phase gate to each LSTM memory unit with learned phase offsets, meaning each memory gate only updated when "in phase" and leading to the equivalent of an attention memory network without the complexity. </p>
<p>In a similar vein to the work using GANs as an input to a loss function for training other networks, my favorite-named paper <a href="https://arxiv.org/abs/1606.04474">Learning to Learn by Gradient Descent by Gradient Descent</a> had a strong showing at the poster session, although I missed their talk (was in the GAN workshop instead).</p>
<p>There was a great talk in the Tensor workshop on how Deep Networks - and in particular CNNs - create exponential expressiveness with only polynomial parameters. This is due to overlapping of the CNN kernels and pooling. He also outlined in this talk how the choice of pooling creates local areas of attention, and alternate choices of pooling can lean to different network topologies able to detect global structure. Additionally, he pointed out that using e.g. CNNs to learn a single binary classifier limits the amount of information that can be learned from any given training sample, resulting in slower convergence and decreased expressiveness.</p>
<p>The workshops seemed to give some actual in-the-wild implementations a chance to present, with an interesting talk from someone at Sandia on using a DNN to compute better estimates for parameters in dynamical systems - in their case fluid flow dynamics. This led to substantial improvements of the simulation over their current default simplifying assumptions, with only a marginal increase in simulation runtime.</p>
<p><a style='text-decoration: none; color: orange;'>
<img src="http://www.mikelanzetta.com/images/bcn-beach.jpg" alt="Beach scene as example of dynamical system"/>
<div style="width:790px;text-align:center;">Another fluid-flow example</div>
</a></p>
<h3>Other Methods</h3>
<p>Variational Inference was big as well, with a great tutorial on it that outlined the method as well as its utility (<a href="http://www.cs.columbia.edu/~blei/talks/2016_NIPS_VI_tutorial.pdf">slides</a>). They walked through the origin of Variational Inference, from the requirement to approximate the Posterior distribution of a predictive model, through the mean-field method, using topic modeling as a motivating example. They then went through scaling VI via stochastic VI, and went deeper with approximating the ELBO via pathwise estimation and several other methods for scaling out VI. Finally, they finished with a deep-dive into better posterior estimation techniques.</p>
<p>Forecasting and Time-Series were not ignored, but still seem like areas where traditional methods (e.g. ARIMA and its descendents) are holding their own. One tutorial went into deep math around why DL methods could forecast well, but was based on (IMHO) the questionable assumption of "smoothness" in the range [T-S, T]. As someone who did Forecasting for Amazon, I can guarantee that the area around Black Friday is anything but differentiable. </p>
<p><a style='text-decoration: none; color: orange;'>
<img src="http://www.mikelanzetta.com/images/smooth-surfaces-were-a-theme.jpg" alt="Rounded hallway in Park Guell"/>
<div style="width:790px;text-align:center;">Smoothness in action</div>
</a></p>
<h2>Conclusion</h2>
<p>I cannot even begin to discuss the full breadth of content presented at NIPS. Machine Learning as a field is expanding far beyond even the specialists' ability to understand all of it, so for a non-theoretician practitioner it's nigh impossible to keep up. <a href="https://tryolabs.com/blog/2016/12/06/major-advancements-deep-learning-2016/">Another article</a> (far better than mine) regarding the advances in the past year just went viral on HN. I'd read that as well to get additional insight. This year at ICML it was quite clear that we didn't even know where the edges were with respect to what DNNs could do. That still seems to be the case at NIPS, and with GANs it's even more evident we still don't know what the limits might be. </p>
<p>Definitely an exciting time to be in the field - I just wish I had more time to keep up. By the end of the conference, my brain felt thoroughly chewed on.</p>
<p><a style='text-decoration: none; color: orange;'>
<img src="http://www.mikelanzetta.com/images/brain-eaten.jpg" alt="Snake-head sculpture appearing to chew on woman's head"/>
<div style="width:490px;text-align:center;">Braiinnnzzz</div>
</a></p><p>There are <a href="http://www.mikelanzetta.com/nips-2016-trip-report.html#disqus_thread">comments</a>.</p>                </article>
            </aside><!-- /#featured -->
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>blogroll</h2>
                        <ul>
                            <li><a href="https://blogs.technet.microsoft.com/machinelearning/">Cortana Analytics</a></li>
                            <li><a href="http://www.kdnuggets.com/">KDNuggets</a></li>
                            <li><a href="http://www.hanselman.com/blog/">Hanselman</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>
                            <li><a href="http://www.mikelanzetta.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate">atom feed</a></li>

                            <li><a href="https://twitter.com/noodlefrenzy">Twitter</a></li>
                            <li><a href="https://www.linkedin.com/in/noodlefrenzy">LinkedIn</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-54597100-1']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = 'https://ssl.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
    </script>
<script type="text/javascript">
    var disqus_shortname = 'intelligentartifice';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = 'https://' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
</body>
</html>